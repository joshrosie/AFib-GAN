{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN83fmXyjmJm"
      },
      "source": [
        "This code references the following tutorials:\n",
        "\n",
        "\n",
        "1.   https://keras.io/examples/generative/conditional_gan/\n",
        "2.   https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "3. https://keras.io/examples/generative/wgan_gp/\n",
        "4. https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/\n",
        "5. https://www.youtube.com/watch?v=IZtv9s_Wx9I&ab_channel=AladdinPersson\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nac1WsLmcjCn"
      },
      "source": [
        "# Requirements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYgcWD_T2Fes"
      },
      "source": [
        "If running the optimizer - functionality is depricated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKCnYdkzZ_f6"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb -qU &> /dev/nul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD9SKO9Vaguq"
      },
      "outputs": [],
      "source": [
        "# import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y4mfBrPaik4"
      },
      "outputs": [],
      "source": [
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApgqEtvc2_cD"
      },
      "outputs": [],
      "source": [
        "!unzip pre_saved_assests/train_final.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove 'other' and erroneous files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCXxmgXCrbu-"
      },
      "outputs": [],
      "source": [
        "!rm -rf content/images_train/other\n",
        "!rm -rf content/images_train/.ipynb_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMMO07v4rwEB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from keras import layers\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from matplotlib import gridspec\n",
        "import keras.backend as K\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import os.path\n",
        "from os import path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These directories are needed for storing the ECGs used for visual inspection as well as the images used for FID and KID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Vu0RT4PXWP"
      },
      "outputs": [],
      "source": [
        "if path.exists('Fake') == False:\n",
        "  os.mkdir('Fake')\n",
        "  os.mkdir('Fake/AF')\n",
        "  os.mkdir('Fake/NORMAL')\n",
        "\n",
        "if path.exists('new_images') == False:\n",
        "  os.mkdir('new_images')\n",
        "  os.mkdir('new_images/AF')\n",
        "  os.mkdir('new_images/NORMAL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rJJLpuP6Jsl"
      },
      "source": [
        "# Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy-1W8joGCvF"
      },
      "source": [
        "Minibatch discrimination layer for Keras which is taken from the following repository:  https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=81de5e4c30810f436dca7b54c5ac3095eeb3ba69&device=unknown&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6761727269646f712f67616e2d67756964652f383164653565346333303831306634333664636137623534633561633330393565656233626136392f4d696e6962617463682532306469736372696d696e6174696f6e2e6970796e62&logged_in=false&nwo=garridoq%2Fgan-guide&path=Minibatch+discrimination.ipynb&platform=android&repository_id=193508311&repository_type=Repository&version=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDrQcVMU4xoL"
      },
      "outputs": [],
      "source": [
        "class MinibatchDiscrimination(tf.keras.layers.Layer): \n",
        "    '''\n",
        "    Minibatch Discrimination custom Keras Layer originally defined in this repositiory:\n",
        "    https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=81de5e4c30810f436dca7b54c5ac3095eeb3ba69&device=unknown&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6761727269646f712f67616e2d67756964652f383164653565346333303831306634333664636137623534633561633330393565656233626136392f4d696e6962617463682532306469736372696d696e6174696f6e2e6970796e62&logged_in=false&nwo=garridoq%2Fgan-guide&path=Minibatch+discrimination.ipynb&platform=android&repository_id=193508311&repository_type=Repository&version=100\n",
        "    '''\n",
        "    def __init__(self, num_kernel, dim_kernel,kernel_initializer='glorot_uniform', **kwargs):\n",
        "        self.num_kernel = num_kernel\n",
        "        self.dim_kernel = dim_kernel\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(input_shape[1], self.num_kernel*self.dim_kernel),\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      trainable=True)\n",
        "        super(MinibatchDiscrimination, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x):\n",
        "        activation = tf.matmul(x, self.kernel)\n",
        "        activation = tf.reshape(activation, shape=(-1, self.num_kernel, self.dim_kernel))\n",
        "        #Mi\n",
        "        tmp1 = tf.expand_dims(activation, 3)\n",
        "        #Mj\n",
        "        tmp2 = tf.transpose(activation, perm=[1, 2, 0])\n",
        "        tmp2 = tf.expand_dims(tmp2, 0)\n",
        "        \n",
        "        diff = tmp1 - tmp2\n",
        "        \n",
        "        l1 = tf.reduce_sum(tf.math.abs(diff), axis=2)\n",
        "        features = tf.reduce_sum(tf.math.exp(-l1), axis=2)\n",
        "        return tf.concat([x, features], axis=1)        \n",
        "        \n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1] + self.num_kernel)\n",
        "      \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['dim_kernel'] =  self.dim_kernel\n",
        "        config['num_kernel'] = self.num_kernel\n",
        "        config[\"kernel_initializer\"] = self.kernel_initializer\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV0bsbO56m2G"
      },
      "outputs": [],
      "source": [
        "def gen_block(x,features,size,stride):\n",
        "    \"\"\"A single convolutional block of the generator.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : Keras.layer object\n",
        "            The previous layer that you wish to add on to.\n",
        "        features : int\n",
        "            Number of features to be used in the convolutional layer.\n",
        "        size : int\n",
        "            Size of the kernel to be used.\n",
        "        stried : int\n",
        "            Stride parameter controls the stride of the Kernel in the convolutional layer\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        x\n",
        "            This function returns the collection of previous layers concatenated to the layer\n",
        "            defined in this function.\n",
        "        \"\"\"\n",
        "    x = layers.Conv2DTranspose(features, kernel_size=size, strides= stride, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
        "    mean=0.0, stddev=0.02), use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkjkfnkleRMl"
      },
      "outputs": [],
      "source": [
        "def define_conditional_generator(n_classes=2, latent_dim = 100):\n",
        "    \"\"\"Used to define the conditional generator for all GAN models\n",
        "\n",
        "        If no arguments are passed, n_classes defaults to 2 and latent_dim\n",
        "        defaults to 100.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_classes : int\n",
        "            Number of classes the generator will be required to\n",
        "            generate (default is 2)\n",
        "\n",
        "        latent_dim : int, optional\n",
        "            The length of the random vector the generator will\n",
        "            use to produce synthetic ECGs (default is 100)\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        model\n",
        "            The conditional generator.\n",
        "        \n",
        "        \"\"\"\n",
        "    label = layers.Input(shape=(1,))\n",
        "    embedding = layers.Embedding(n_classes, latent_dim)(label) #embedding layer based on size of latent vector and number of classes\n",
        "    dense = layers.Dense(4*4*1)(embedding) # additional dense layer\n",
        "    label_output = layers.Reshape((4, 4, 1))(dense) # As with the original DCGAN - we start with a 4x4x1 (https://doi.org/10.48550/arxiv.1511.06434)\n",
        "\n",
        "    latent_vector = layers.Input(shape=(latent_dim,))\n",
        "    latent_dense = layers.Dense(512 * 4 * 4)(latent_vector)\n",
        "    latent_dense = layers.ReLU()(latent_dense)\n",
        "    latent_vector_output = layers.Reshape((4, 4, 512))(latent_dense) # unlike Radford et al., we take an approach more similar to Li et al. \n",
        "    #(https://www.sciencedirect.com/science/article/pii/S0020025521013049) and reduce the number of features\n",
        "\n",
        "    concat = layers.Concatenate()([latent_vector_output, label_output])\n",
        "\n",
        "    x = gen_block(concat,512,4,2)\n",
        "\n",
        "    x = gen_block(x,256,4,2)\n",
        "\n",
        "    x = gen_block(x,128,4,2)\n",
        "\n",
        "    x = gen_block(x,64,4,2)\n",
        "    \n",
        "    x = layers.Conv2DTranspose(1, 4, 2,padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
        "    mean=0.0, stddev=0.02), use_bias=False, activation='tanh')(x)\n",
        "    \n",
        "    model = tf.keras.Model([latent_vector,  label], x)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-7xoClt5oBc"
      },
      "outputs": [],
      "source": [
        "def disc_block(x,features,size,stride,clip = False, gp=False):\n",
        "    \"\"\"A single convolutional block of the discriminator/critic.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : Keras.layer object\n",
        "            The previous layer/s that you wish to add on to.\n",
        "        features : int\n",
        "            Number of features to be used in the convolutional layer.\n",
        "        size : int\n",
        "            Size of the kernel to be used.\n",
        "        stried : int\n",
        "            Stride parameter controls the stride of the Kernel in the \n",
        "            convolutional layer\n",
        "        clip : bool\n",
        "            Activates the norm clipping functionality. Set to True if this \n",
        "            block is to be used in the WCGAN. \n",
        "        gp :\n",
        "            Removes batch normalization. Set to True if the block is to be\n",
        "            used in the WCGANGP.\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        x\n",
        "            This function returns the collection of previous layers concatenated to the layer\n",
        "            defined in this function.\n",
        "    \"\"\"\n",
        "    if not clip: \n",
        "      x = layers.Conv2D(features, kernel_size=size, strides= stride, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
        "      mean=0.0, stddev=0.02), use_bias=False)(x)\n",
        "    else: # for WCGAN, makes use of Keras's kernal constraint functionality\n",
        "      x = layers.Conv2D(features,  kernel_size=size, strides= stride, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
        "      mean=0.0, stddev=0.02), use_bias=False, kernel_constraint=keras.constraints.min_max_norm(-clip,clip, axis =[0,1,2]))(x)\n",
        "    if not gp: # we do not use batch normalization for the WCGANGP\n",
        "      x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJzZIcsreIyE"
      },
      "outputs": [],
      "source": [
        "def define_conditional_discriminator(in_shape=(128,128,1), n_classes=2, embedding_dim=100):\n",
        "    \"\"\"Used to define the conditional discriminator for the DCCGAN\n",
        "\n",
        "        If no arguments are passed, input_shape defualts to (128,128,1),n_classes defaults defaults to 2, embedding_dim\n",
        "        defaults to 100.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_shape : 3-tuple\n",
        "            Shape of the real image (this shape must be the same as the image\n",
        "            being generated). (default is (128,128,1))\n",
        "\n",
        "        n_classes : int\n",
        "            Number of classes the generator will be required to\n",
        "            generate (default is 2)\n",
        "\n",
        "        latent_dim : int, optional\n",
        "            The length of the random vector the generator will\n",
        "            use to produce synthetic ECGs (default is 100)\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        model\n",
        "            The conditional discriminator to be used with the DCCGAN.\n",
        "        \n",
        "    \"\"\"\n",
        "    label = layers.Input(shape=(1,))\n",
        "    embedding = layers.Embedding(n_classes, embedding_dim)(label)\n",
        "    dense = layers.Dense(in_shape[0] * in_shape[1] * in_shape[2])(embedding)\n",
        "    condition = layers.Reshape((in_shape[0], in_shape[1], in_shape[2]))(dense)\n",
        "    image = layers.Input(shape=in_shape)\n",
        "    concat = layers.Concatenate()([image, condition])\n",
        "    \n",
        "    noise = layers.GaussianNoise(0.1)(concat) # added Gaussian Noise layer helps avoid mode collapse.\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
        "    mean=0.0, stddev=0.02), use_bias=False)(noise)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = disc_block(x,128,4,3)\n",
        "\n",
        "    x = disc_block(x,256,4,3)\n",
        "\n",
        "    x = disc_block(x,512,4,3)\n",
        "\n",
        "    x = layers.Flatten()(x) \n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model([image, label], x)\n",
        "    \n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjLNkjDdBqIK"
      },
      "outputs": [],
      "source": [
        "def define_conditional_critic(clip,in_shape=(128,128,1), n_classes=2, embedding_dim=100):\n",
        "    \"\"\"Used to define the conditional critic for the WCGAN\n",
        "\n",
        "        If no arguments are passed, input_shape defualts to (128,128,1),n_classes defaults defaults to 2, embedding_dim\n",
        "        defaults to 100.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        clip : int\n",
        "            A number by which the norms of the gradient will be resticted by.\n",
        "            Norms will be restricted into the range of (-clip,clip)\n",
        "\n",
        "        in_shape : 3-tuple\n",
        "            Shape of the real image (this shape must be the same as the image\n",
        "            being generated). (default is (128,128,1))\n",
        "\n",
        "        n_classes : int\n",
        "            Number of classes the generator will be required to\n",
        "            generate (default is 2)\n",
        "\n",
        "        latent_dim : int, optional\n",
        "            The length of the random vector the generator will\n",
        "            use to produce synthetic ECGs (default is 100)\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        model\n",
        "            The conditional discriminator to be used with the WCGAN.\n",
        "        \"\"\"\n",
        "    label = layers.Input(shape=(1,))\n",
        "    embedding = layers.Embedding(n_classes, embedding_dim)(label)\n",
        "    dense = layers.Dense(in_shape[0] * in_shape[1] * in_shape[2])(embedding)\n",
        "    condition = layers.Reshape((in_shape[0], in_shape[1], 1))(dense)\n",
        "    image = layers.Input(shape=in_shape)\n",
        "    concat = layers.Concatenate()([image, condition])\n",
        "    \n",
        "    noise = layers.GaussianNoise(1)(concat) # added Gaussian Noise layer helps avoid mode collapse.\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
        "    mean=0.0, stddev=0.02),  use_bias=False, kernel_constraint=keras.constraints.min_max_norm(-clip,clip, axis =[0,1,2]) )(noise)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "\n",
        "    x = disc_block(x,128,4,3,clip = True)\n",
        "    x = disc_block(x,256,4,3,clip = True)\n",
        "    x = disc_block(x,512,4,3,clip = True)\n",
        "\n",
        "    flatten = layers.Flatten()(x) #3D to 2D\n",
        "    dropout = layers.Dropout(0.4)(flatten)\n",
        "    dense = layers.Dense(1)(dropout)\n",
        "\n",
        "    model = tf.keras.Model([image, label], dense)\n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9BzaPdaeXhQ"
      },
      "outputs": [],
      "source": [
        "def define_conditional_critic_gp(in_shape=(128,128,1), n_classes=3, embedding_dim=100):\n",
        "    \"\"\"Used to define the conditional discriminator for the DCCGAN\n",
        "\n",
        "        If no arguments are passed, input_shape defualts to (128,128,1),n_classes defaults defaults to 2, embedding_dim\n",
        "        defaults to 100.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_shape : 3-tuple\n",
        "            Shape of the real image (this shape must be the same as the image\n",
        "            being generated). (default is (128,128,1))\n",
        "\n",
        "        n_classes : int\n",
        "            Number of classes the generator will be required to\n",
        "            generate (default is 2)\n",
        "\n",
        "        latent_dim : int, optional\n",
        "            The length of the random vector the generator will\n",
        "            use to produce synthetic ECGs (default is 100)\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        model\n",
        "            The conditional discriminator to be used with the WCGANGP.\n",
        "    \"\"\"\n",
        "\n",
        "    label = layers.Input(shape=(1,))\n",
        "    embedding = layers.Embedding(n_classes, embedding_dim)(label)\n",
        "    dense = layers.Dense(in_shape[0] * in_shape[1] * in_shape[2])(embedding)\n",
        "    condition = layers.Reshape((in_shape[0], in_shape[1], in_shape[2]))(dense)\n",
        "    image = layers.Input(shape=in_shape)\n",
        "    concat = layers.Concatenate()([image, condition])\n",
        "    \n",
        "    x = disc_block(concat, 64,4,2,gp = True)\n",
        "\n",
        "    x = disc_block(x, 128,4,3,gp = True)\n",
        "\n",
        "    x = disc_block(x, 256,4,3,gp = True)\n",
        "\n",
        "    x = disc_block(x, 512,4,3,gp = True)\n",
        "\n",
        "    flatten = layers.Flatten()(x) #3D to 2D\n",
        "    # mbd = MinibatchDiscrimination(num_kernel=100, dim_kernel=2)(flatten)\n",
        "    dropout = layers.Dropout(0.5)(flatten)\n",
        "    dense = layers.Dense(1)(dropout)\n",
        "\n",
        "    model = tf.keras.Model([image, label], dense)\n",
        "    return model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZAybgjaczGv"
      },
      "source": [
        "# TrainGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcn57nm5efCw"
      },
      "outputs": [],
      "source": [
        "class trainGAN():\n",
        "  '''\n",
        "  A class used to run experiments involving GANs.\n",
        "  Attributes\n",
        "  ----------\n",
        "  dataset : str\n",
        "    The name of a tf.data.Dataset object\n",
        "\n",
        "  optimizer : str\n",
        "    The name of a an optimizer: adam, rms, nadam.\n",
        "    Name is coverted to a tf.keras.optimizers object of the same name.\n",
        "\n",
        "  num_classes : int\n",
        "    The number of classes being represented. Should equal the number of classes\n",
        "    in the dataset. For AF, NORMAL, and OTHER, this should be 3. For AF and \n",
        "    NORMAL only, this should be 2.\n",
        "\n",
        "  embedding_dim : int\n",
        "    The column dimensions of the embedding layer. Embedding layer has the total\n",
        "    dimensions = num_classes * embedding_dim.\n",
        "\n",
        "  latenet_dim : int\n",
        "    The length of the latenet vector being supplied to the generator.\n",
        "\n",
        "  batch_size : int\n",
        "    The batch size used to read in the dataset.\n",
        "\n",
        "  learning_rate:\n",
        "    A hyperparameter controlling the learning rate used in the optimizer.\n",
        "  \n",
        "  epochs : int\n",
        "    The number of training cycles\n",
        "  \n",
        "  n_critic : int\n",
        "    The number of iterations the critic will be trained for every train step.\n",
        "    Applies only to wcgan and wcgangp.\n",
        "\n",
        "  beta : float\n",
        "    The Beta 1 parameter for Adam and Nadam\n",
        "  \n",
        "  gan_type : str\n",
        "    dccgan, wcgan, or wcgangp. Relates to the loss function and architectures\n",
        "    to be used in GAN training\n",
        "\n",
        "  clip : int\n",
        "  The limit by which the norm of the gradients of the critc are clipped in the \n",
        "  case of the wgan. To ensure the lipschitz 1 constraint is obeyed, this value\n",
        "  must be set to 1.\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "\n",
        "  generator_loss(label : Tensor, image : Tensor)\n",
        "      Used to define the cross entropy loss function of the generator.\n",
        "\n",
        "  discriminator_loss(label : Tensor, image : Tensor)\n",
        "      Used to define the cross entropy loss function of the discriminator.\n",
        "\n",
        "  normalization(image : Tensor)\n",
        "      Used to normalize a tensor containing an image between (-1,1).\n",
        "\n",
        "  gradient_penalty(target : Tensor, real_imgs : Tensor, fake_imgs : Tensor)\n",
        "      Used to calculate a gradient penalty for a particular interpolation\n",
        "      of a set of real and fake images.\n",
        "\n",
        "  label_maker(n_classes : int ,ecg_type='NORMAL' : str, num_eg=16 : int)\n",
        "      A helper function used to generate a set of labels (NORMAL,AF,OTHER)\n",
        "      to be used in image generation functions. Generates a tensor of zero, one, or \n",
        "      two equalling length of the num_eg parameter.\n",
        "\n",
        "  generate_and_save_images(model : Model, epoch : int, seed : Tensor, ecg_type = 'NORMAL' : str)\n",
        "      A function used to generate and save a set of images to be used to \n",
        "      assess GAN performance from a visual quality standpoint.\n",
        "\n",
        "  generate_new_images(model : Model, num_eg : int, directory='new_images' : str ,ecg_type = 'NORMAL' : str, batch_size=1024 : int)\n",
        "      A function used to generate and save a set of images to be used to \n",
        "      assess GAN performance using Frechét Inception Distance and Kernel\n",
        "      Inception Distance. The clean-fid package requires that images are saved\n",
        "      in a directory.\n",
        "\n",
        "  train_step_dcgan(images : Tensor, target : Tensor)\n",
        "      A single training step to be used in conjunction with the DCCGAN.\n",
        "      This method will NOT combine the loss of the discriminator on real and fake\n",
        "      samples.\n",
        "\n",
        "  train_step_dcgan_combine(images : Tensor, target : Tensor)\n",
        "      A single training step to be used in conjunction with the DCCGAN.\n",
        "      This method will combine the loss of the discriminator on real and fake\n",
        "      samples.\n",
        "\n",
        "  train_step_wgan(images : Tensor, target : Tensor)\n",
        "      A single training step to be used in conjunction with the WCGAN.\n",
        "\n",
        "  train_step_wgangp(images : Tensor, target : Tensor)\n",
        "      A single training step to be used in conjunction with the WCGANGP.\n",
        "\n",
        "  train()\n",
        "      The main training loop used for all GAN model.Calls a train_step \n",
        "      function deterimend by the gan_type parameter.\n",
        "  \n",
        "  '''\n",
        "  def __init__(self, dataset, optimizer,num_classes, embedding_dim, latent_dim, batch_size, \n",
        "               learning_rate, epochs, num_eg=16,n_critic = 3,beta = 0.5, gan_type = 'dccgan' ,clip = 1):\n",
        "      \n",
        "      self.batch_size = batch_size\n",
        "      self.gan_type = gan_type\n",
        "      self.gp_weight=10.0\n",
        "      self.dataset = dataset\n",
        "      \n",
        "    \n",
        "      self.num_classes = num_classes\n",
        "      self.embedding_dim = embedding_dim\n",
        "      self.latent_dim = latent_dim\n",
        "      self.learning_rate = learning_rate\n",
        "      self.epochs = epochs\n",
        "      self.n_critic = n_critic\n",
        "      self.clip = clip\n",
        "\n",
        "      self.binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "      self.num_eg = 16\n",
        "      self.seed = tf.random.normal([num_eg, self.latent_dim])\n",
        "      \n",
        "      self.gen = define_conditional_generator(n_classes = num_classes, latent_dim=self.latent_dim)\n",
        "      \n",
        "      if optimizer == 'adam':\n",
        "          self.generator_opt = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = beta, beta_2 = 0.999 )\n",
        "          self.disc_opt = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = beta, beta_2 = 0.999 )\n",
        "\n",
        "      if optimizer == 'rms':\n",
        "          self.generator_opt = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
        "          self.disc_opt = tf.keras.optimizers.RMSprop(learning_rate = learning_rate)\n",
        "\n",
        "      if optimizer == 'nadam':\n",
        "          self.generator_opt = tf.keras.optimizers.Nadam(learning_rate = learning_rate, beta_1 = beta, beta_2 = 0.999 )\n",
        "          self.disc_opt= tf.keras.optimizers.Nadam(learning_rate = learning_rate, beta_1 = beta, beta_2 = 0.999 )\n",
        "      \n",
        "      if gan_type == 'dccgan':\n",
        "          self.disc = define_conditional_discriminator(n_classes=num_classes, embedding_dim=self.embedding_dim)\n",
        "         \n",
        "      if gan_type == 'wcgan':\n",
        "          self.disc = define_conditional_critic(self.clip, n_classes=num_classes, embedding_dim=self.embedding_dim)\n",
        "\n",
        "      if gan_type == 'wcgangp':\n",
        "          self.disc = define_conditional_critic_gp(n_classes=num_classes, embedding_dim=self.embedding_dim)\n",
        "      \n",
        "     \n",
        "      self.checkpoint_dir = './checkpoints'\n",
        "      self.checkpoint_prefix = os.path.join(self.checkpoint_dir, \"ckpt\")\n",
        "      self.checkpoint = tf.train.Checkpoint(generator_opt=self.generator_opt,\n",
        "                                disc_opt=self.disc_opt,\n",
        "                                generator=self.gen,\n",
        "                                discriminator=self.disc)\n",
        "\n",
        "  def generator_loss(self,label, image):\n",
        "      \"\"\"Used to define the cross entropy loss function of the generator.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        label : tf.keras.Tensor\n",
        "            A tensor of all ones indicating that the associated image is real\n",
        "            when it is in fact fake\n",
        "\n",
        "        image : tf.keras.Tensor\n",
        "            A tensor containing the generated image.\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        gen_loss\n",
        "            The result of applying binary cross entropy loss via the \n",
        "            discriminator.\n",
        "        \"\"\"\n",
        "      gen_loss = self.binary_cross_entropy(label, image)\n",
        "      return gen_loss\n",
        "\n",
        "  def discriminator_loss(self,label, image):\n",
        "      \"\"\"Used to define the cross entropy loss function of the discriminator.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      label : tf.keras.Tensor\n",
        "        A tensor of either all ones indicating that the associated image is \n",
        "        real, or all zeroes indicating that the image is fake.\n",
        "        \n",
        "      image : tf.keras.Tensor\n",
        "        A tensor containing the generated image.\n",
        "\n",
        "      Returns\n",
        "      ------\n",
        "      gen_loss\n",
        "        The result of applying binary cross entropy loss via the \n",
        "        discriminator.\n",
        "      \"\"\"\n",
        "      disc_loss = self.binary_cross_entropy(label, image)\n",
        "      return disc_loss\n",
        "\n",
        "  @tf.function\n",
        "  def normalization(self,image):\n",
        "      \"\"\"Used to normalize a tensor containing an image between (-1,1).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        image : tf.keras.Tensor\n",
        "            A tensor containing the image/s to be normalized.\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        image\n",
        "            The same tensor normalized between (-1,1)\n",
        "\n",
        "        References: \n",
        "        -----------\n",
        "            https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/\n",
        "        \n",
        "      \"\"\"\n",
        "      image = tf.image.resize(image, (128,128))\n",
        "      image = tf.subtract(tf.divide(image, 127.5),1) \n",
        "      return image\n",
        "\n",
        "############\n",
        "  def gradient_penalty(self, target, real_imgs, fake_imgs):\n",
        "      \"\"\"Used to calculate a gradient penalty for a particular interpolation\n",
        "      of a set of real and fake images.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        target : tf.keras.Tensor\n",
        "            A tensor of zeroe or one or two indicating that the associated real and genrated \n",
        "            images display AF or Normal or OTHER.\n",
        "\n",
        "        real_imgs : tf.keras.Tensor\n",
        "            A tensor containing real images.\n",
        "        \n",
        "        fake_imgs : tf.keras.Tensor\n",
        "            A tensor containing fake images.\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        gp\n",
        "            The resulting gradient penalty calculated over the set of real \n",
        "            and syntehtic images.\n",
        "        \n",
        "        References: \n",
        "        -----------\n",
        "           https://keras.io/examples/generative/wgan_gp/\n",
        "      \"\"\"\n",
        "      epsilon = tf.random.uniform((real_imgs.shape[0],1,1,1))\n",
        "      interpolated = real_imgs * epsilon + fake_imgs * (1-epsilon)\n",
        "      with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            pred = self.disc([interpolated,target], training=True)\n",
        "      grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "      norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "      gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "      return gp\n",
        "\n",
        "#############\n",
        "\n",
        "  def label_maker(self,n_classes,ecg_type='NORMAL', num_eg=16):\n",
        "      \"\"\"A helper function used to generate a set of labels (NORMAL,AF,OTHER)\n",
        "      to be used in image generation functions. Generates a tensor of zero, one, or \n",
        "      two equalling length of the num_eg parameter.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_classes : int\n",
        "            not necessary\n",
        "\n",
        "        ecg_type : str\n",
        "            A string indicating the type of ECG to generate. Accepted arguments\n",
        "            are 'NORMAL', 'AF', or 'OTHER'.\n",
        "        \n",
        "        num_eg : int\n",
        "            The number of labels to generate\n",
        "\n",
        "        Returns\n",
        "        ------\n",
        "        labels\n",
        "            A tensor of labels of either 0,1 or 2 corresponding to AF, NORMAL or\n",
        "            OTHER\n",
        "        References: \n",
        "        -----------\n",
        "            https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/\n",
        "      \"\"\"\n",
        "      if ecg_type == 'AF':\n",
        "          lab = tf.cast(0,  dtype=tf.dtypes.int32)\n",
        "      elif ecg_type == 'NORMAL':\n",
        "          lab = tf.cast(1,  dtype=tf.dtypes.int32)\n",
        "      else:\n",
        "          lab = tf.cast(2,  dtype=tf.dtypes.int32)\n",
        "      labels = tf.repeat(lab, [num_eg], axis=None, name=None)\n",
        "\n",
        "      return labels\n",
        "\n",
        "  def generate_and_save_images(self,model, epoch, seed, ecg_type = 'NORMAL'):\n",
        "      \"\"\"A function used to generate and save a set of images to be used to \n",
        "      assess GAN performance from a visual quality standpoint.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      model : tf.Model\n",
        "      The generator to be used to generate images.\n",
        "\n",
        "      epoch : int\n",
        "      The current epoch of training\n",
        "\n",
        "      seed : tf.Tensor\n",
        "      A fixed tensor of shape num_eg*latent_dim containing random numbers\n",
        "      drawn from the Gaussian distribution where num_eg is the required\n",
        "      number of fake ECGs to generate.\n",
        "\n",
        "      ecg_type : str\n",
        "      A string indicating the type of ECG to generate. Accepted arguments\n",
        "      are 'NORMAL', 'AF', or 'OTHER'. Used in conjunction with the label_gen\n",
        "      function to generate a tenesor of integers corresponding to the\n",
        "      chosen ECG type.\n",
        "      References: \n",
        "      -----------\n",
        "      https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/\n",
        "      \"\"\"\n",
        "      labels = self.label_maker(n_classes=2, ecg_type = ecg_type)\n",
        "      predictions = model([seed, labels], training=False)\n",
        "      \n",
        "      for i in range(predictions.shape[0]):\n",
        "          plt.subplot(4, 4, i+1)\n",
        "          pred = (predictions[i, :, :, :] + 1 ) * 127.5\n",
        "          pred = np.array(pred)  \n",
        "          img = keras.preprocessing.image.array_to_img(pred)\n",
        "          plt.imshow(img, cmap='gray')\n",
        "          plt.axis('off')\n",
        "          \n",
        "\n",
        "           # img.save(\"Fake/{ecg}/generated_img_{i}_{epoch}.png\".format(ecg = ecg_type,i=i, epoch=epoch))\n",
        "      plt.savefig('Fake/' + ecg_type +'/image_at_epoch_{:d}.png'.format(epoch))\n",
        "\n",
        "\n",
        "          \n",
        "  def generate_new_images(self, model, num_eg, directory='new_images',ecg_type = 'NORMAL', batch_size=1024):\n",
        "      \"\"\"A function used to generate and save a set of images to be used to \n",
        "      assess GAN performance using Frechét Inception Distance and Kernel\n",
        "      Inception Distance. The clean-fid package requires that images are saved\n",
        "      in a directory.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      model : tf.Model\n",
        "      The generator to be used to generate images.\n",
        "\n",
        "      num_eg : int\n",
        "      The total number of ECGs that is required to generate\n",
        "\n",
        "      directory : string\n",
        "      The directory to which the new images are saved to (defaults to\n",
        "      'new_images')\n",
        "\n",
        "      ecg_type : str\n",
        "      A string indicating the type of ECG to generate. Accepted arguments\n",
        "      are 'NORMAL', 'AF', or 'OTHER'. Used in conjunction with the label_gen\n",
        "      function to generate a tenesor of integers corresponding to the\n",
        "      chosen ECG type.\n",
        "\n",
        "      batch_size : int\n",
        "      This determines how many images will be generated in a single iteration\n",
        "      to reduce memory requirements.\n",
        "      \"\"\"\n",
        "      limit = num_eg//batch_size\n",
        "      # self.checkpoint.restore(tf.train.latest_checkpoint(t.checkpoint_dir))\n",
        "      count = 0\n",
        "      if num_eg%batch_size!=0:\n",
        "        print(\"please ensure batch size and number of examples are divisible\")\n",
        "        return\n",
        "      \n",
        "      for i in range(limit):\n",
        "        input = tf.random.normal([batch_size, self.latent_dim])\n",
        "        labels = self.label_maker(n_classes=self.num_classes, ecg_type = ecg_type, num_eg = batch_size)\n",
        "\n",
        "        predictions = model([input, labels], training=False)\n",
        "        \n",
        "        for j in range(predictions.shape[0]):\n",
        "      \n",
        "            pred = (predictions[j, :, :, :] + 1 ) * 127.5\n",
        "            pred = np.asarray(pred)  \n",
        "            img = keras.preprocessing.image.array_to_img(pred)\n",
        "            img.save(\"{directory}/{ecg}/generated_{ecg}_{i}.png\".format(directory=directory, ecg = ecg_type,i=count))\n",
        "            count += 1\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def train_step_test(self,images,target):\n",
        "      \n",
        "      noise = tf.random.normal([target.shape[0], self.latent_dim])\n",
        "     \n",
        "      with tf.GradientTape() as disc_tape_real:\n",
        "          generated_images = self.gen([noise,target], training=True)\n",
        "\n",
        "          real_output = self.disc([images,target], training=True)\n",
        "          real_targets = tf.ones_like(real_output)\n",
        "          disc_loss_real = self.discriminator_loss(real_targets, real_output)\n",
        "\n",
        "          fake_output = self.disc([generated_images,target], training=True)\n",
        "          fake_targets = tf.zeros_like(fake_output)\n",
        "          disc_loss_fake = self.discriminator_loss(fake_targets, fake_output)\n",
        "\n",
        "          disc_loss = 0.5*(disc_loss_real+disc_loss_fake)\n",
        "\n",
        "      gradients_of_disc_real = disc_tape_real.gradient(disc_loss, self.disc.trainable_variables)\n",
        "      self.disc_opt.apply_gradients(zip(gradients_of_disc_real,self.disc.trainable_variables))\n",
        "      \n",
        "\n",
        "      with tf.GradientTape() as gen_tape:\n",
        "          generated_images = self.gen([noise,target], training=True)\n",
        "          fake_output = self.disc([generated_images,target], training=True)\n",
        "          real_targets = tf.ones_like(fake_output)\n",
        "          gen_loss = self.generator_loss(real_targets, fake_output)\n",
        "\n",
        "      gradients_of_gen = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
        "      self.generator_opt.apply_gradients(zip(gradients_of_gen,self.gen.trainable_variables))\n",
        "\n",
        "      return  disc_loss_real,  disc_loss_fake, gen_loss\n",
        "        \n",
        "  \n",
        "\n",
        "  @tf.function\n",
        "  def train_step_dcgan(self,images,target): \n",
        "      \"\"\"A single training step to be used in conjunction with the DCCGAN.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        images : tf.Tensor\n",
        "            A tensor of real images\n",
        "\n",
        "        target : tf.Tensor\n",
        "            A list of numbers corresponding to the true label of each image\n",
        "      \"\"\"\n",
        "      noise = tf.random.normal([target.shape[0], self.latent_dim])\n",
        "     \n",
        "      with tf.GradientTape() as real_tape, tf.GradientTape() as fake_tape: # Soumith\n",
        "          generated_images = self.gen([noise,target], training=True)\n",
        "          \n",
        "          #give real images with real label\n",
        "          real_guess = self.disc([images,target], training=True)\n",
        "          real_ans = tf.ones_like(real_guess)\n",
        "          real_loss = self.discriminator_loss(real_ans, real_guess)\n",
        "\n",
        "          #give fake images with fake label\n",
        "          fake_guess = self.disc([generated_images,target], training=True)\n",
        "          fake_ans = tf.zeros_like(fake_guess)\n",
        "          fake_loss = self.discriminator_loss(fake_ans, fake_guess)\n",
        "\n",
        "          disc_loss = 0.5*(real_loss+fake_loss)\n",
        "\n",
        "      grad_disc_real = real_tape.gradient(real_loss, self.disc.trainable_variables)\n",
        "      self.disc_opt.apply_gradients(zip(grad_disc_real,self.disc.trainable_variables))\n",
        "      \n",
        "      grad_disc_fake = fake_tape.gradient(fake_loss, self.disc.trainable_variables)\n",
        "      self.disc_opt.apply_gradients(zip(grad_disc_fake,self.disc.trainable_variables))\n",
        "\n",
        "      with tf.GradientTape() as gen_tape:\n",
        "          generated_images = self.gen([noise,target], training=True)\n",
        "          #give fake images with real label\n",
        "          fake_guess = self.disc([generated_images,target], training=True)\n",
        "          real_answer = tf.ones_like(fake_guess)\n",
        "          gen_loss = self.generator_loss(real_answer, fake_guess)\n",
        "\n",
        "      grad_gen = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
        "      self.generator_opt.apply_gradients(zip(grad_gen,self.gen.trainable_variables))\n",
        "\n",
        "      return  real_loss,  fake_loss, gen_loss\n",
        "\n",
        " \n",
        "  \n",
        "  @tf.function\n",
        "  def train_step_wgan(self,images,target):\n",
        "\n",
        "    \"\"\"A single training step to be used in conjunction with the WCGAN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    images : tf.Tensor\n",
        "        A tensor of real images\n",
        "\n",
        "    target : tf.Tensor\n",
        "        A list of numbers corresponding to the true label of each image\n",
        "    \"\"\"\n",
        "    for _ in range(self.n_critic):\n",
        "      noise = tf.random.normal([target.shape[0], self.latent_dim])\n",
        "\n",
        "      with tf.GradientTape() as disc_tape:\n",
        "        generated_images = self.gen([noise,target], training=True)\n",
        "        real_guess = self.disc([images,target], training=True)\n",
        "        fake_guess = self.disc([generated_images,target], training=True)\n",
        "        disc_loss = tf.math.reduce_mean(fake_guess) - tf.math.reduce_mean(real_guess)\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
        "    self.disc_opt.apply_gradients(zip(gradients_of_disc,self.disc.trainable_variables))\n",
        "\n",
        "    noise = tf.random.normal([target.shape[0], self.latent_dim])\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      generated_images = self.gen([noise,target], training=True)\n",
        "      fake_guess = self.disc([generated_images,target], training=True)\n",
        "      gen_loss = -tf.math.reduce_mean(fake_guess)   \n",
        "    gradients_of_gen = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
        "    self.generator_opt.apply_gradients(zip(gradients_of_gen,self.gen.trainable_variables))\n",
        "\n",
        "    return disc_loss,gen_loss           \n",
        "\n",
        "  @tf.function\n",
        "  def train_step_wgan_gp(self,images,target):\n",
        "      \"\"\"A single training step to be used in conjunction with the WCGANGP.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      images : tf.Tensor\n",
        "          A tensor of real images\n",
        "\n",
        "      target : tf.Tensor\n",
        "          A list of numbers corresponding to the true label of each image\n",
        "      \"\"\"\n",
        "      for _ in range(self.n_critic):\n",
        "        noise = tf.random.normal([target.shape[0], self.latent_dim])\n",
        "        \n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            generated_images = self.gen([noise,target], training=True)\n",
        "            real_guess = self.disc([images,target], training=True)\n",
        "            fake_guess = self.disc([generated_images,target], training=True)\n",
        "            disc_loss = tf.math.reduce_mean(fake_guess) - tf.math.reduce_mean(real_guess)\n",
        "            gp = self.gradient_penalty(target,images, generated_images)\n",
        "            disc_loss = disc_loss + gp * 10\n",
        "            \n",
        "        gradients_of_disc = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
        "        self.disc_opt.apply_gradients(zip(gradients_of_disc,self.disc.trainable_variables))\n",
        "        \n",
        "      noise = tf.random.normal([target.shape[0], self.latent_dim])\n",
        "      with tf.GradientTape() as gen_tape:\n",
        "          generated_images = self.gen([noise,target], training=True)\n",
        "          fake_output = self.disc([generated_images,target], training=True)\n",
        "          gen_loss = -tf.math.reduce_mean(fake_output)   \n",
        "      gradients_of_gen = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
        "      self.generator_opt.apply_gradients(zip(gradients_of_gen,self.gen.trainable_variables))\n",
        "       \n",
        "      return disc_loss,gen_loss\n",
        "  \n",
        "     \n",
        "\n",
        "  def train(self):\n",
        "      \"\"\"The main training loop used for all GAN model.Calls a train_step \n",
        "      function deterimend by the gan_type parameter.\n",
        "      \"\"\"\n",
        "      disc_losses = []\n",
        "      gen_losses = []\n",
        "      real_embeddings = []\n",
        "      fake_embeddings = []\n",
        "      disc_loss = None\n",
        "      gen_loss = None\n",
        "      x = None\n",
        "      y = None\n",
        "      fid_list = []\n",
        "\n",
        "      for epoch in range(self.epochs):\n",
        "          start = time.time()\n",
        "         \n",
        "          count = 0\n",
        "          for image_batch,target in tqdm(self.dataset):\n",
        "              \n",
        "              img = tf.cast(image_batch, tf.float32)\n",
        "              imgs = self.normalization(img)\n",
        "              \n",
        "              if self.gan_type == 'dccgan':\n",
        "                disc1,disc2,gen_loss = self.train_step_dcgan(imgs,target)\n",
        "                disc_loss = (disc1+disc2)/2\n",
        "\n",
        "              if self.gan_type == 'wcgan':\n",
        "                  disc_loss,gen_loss = self.train_step_wgan(imgs,target)\n",
        "\n",
        "              if self.gan_type == 'wcgangp':\n",
        "                disc_loss,gen_loss = self.train_step_wgan_gp(imgs,target)\n",
        "\n",
        "\n",
        "\n",
        "          self.generate_and_save_images(self.gen,\n",
        "                              epoch + 1,\n",
        "                              self.seed,\n",
        "                               'NORMAL')\n",
        "\n",
        "          self.generate_and_save_images(self.gen,\n",
        "                              epoch + 1,\n",
        "                              self.seed,\n",
        "                              'AF')\n",
        "          \n",
        "          \n",
        "          # af_img = wandb.Image('Fake/AF/image_at_epoch_{:d}.png'.format(epoch+1), caption=\"AF image\")\n",
        "          # normal_img = wandb.Image('Fake/NORMAL/image_at_epoch_{:d}.png'.format(epoch+1), caption=\"Normal image\")\n",
        "          \n",
        "          if epoch % 10 == 0 or epoch == 0:\n",
        "              print('Saving...')\n",
        "              self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
        "          \n",
        "\n",
        "          sess = tf.compat.v1.Session()\n",
        "          with sess.as_default(): \n",
        "              print(disc_loss.numpy())\n",
        "              disc_losses.append(disc_loss.numpy())\n",
        "              gen_losses.append(gen_loss.numpy())\n",
        "\n",
        "              \n",
        "          print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "          \n",
        "          # wandb.log({'disc_loss': np.mean(disc_losses), \n",
        "          #       'gen_loss': np.mean(gen_losses),\n",
        "          #       'is_mu':float(x),\n",
        "          #       'is_sigma':float(y),\n",
        "          #       'fid': fid,\n",
        "          #       'af_img' : af_img,\n",
        "          #       'normal_img': normal_img})\n",
        "      # # return disc_losses,gen_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCOBDBMcc-Vs"
      },
      "source": [
        "# Use Cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to read in datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlHUP53txcoU"
      },
      "outputs": [],
      "source": [
        "# RUN ME\n",
        "def load_grayscale_images(dir, in_shape = (128,128), batch_size = 64):\n",
        "    \"\"\"Helper function used to read in an image dataset from directory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dir : str\n",
        "        Directory of images. Labels correspond to subfolders.\n",
        "        \n",
        "    in_shape : tuple\n",
        "        Describes the x,y dimentions of a given images (defaults to (128,128)).\n",
        "\n",
        "    batch_size : int\n",
        "        The number of images to be read in from the directory at any call to this dataset object.\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "    dataset\n",
        "        A tf.data.Dataset object.\n",
        "    \"\"\"\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "          dir, \n",
        "          labels=\"inferred\",\n",
        "          label_mode=\"int\",\n",
        "          image_size = in_shape, \n",
        "          batch_size = batch_size,\n",
        "          color_mode = \"grayscale\"\n",
        "        )\n",
        "    return dataset\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TANZ6wiioBFa"
      },
      "source": [
        "### Select the experiment you want to run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In2j17tUhpBo"
      },
      "source": [
        "## AFib-GAN:\n",
        "- Batch Size = 64\n",
        "- Optimizer = Adam\n",
        "- Learning Rate = 0.0002\n",
        "- Embedding Dimension = 100\n",
        "- Latent Dimennsion = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "TOZ2ex1Bel0W",
        "outputId": "d1b353a4-a9c4-416e-dd1f-1f3c628fd4bf"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BATCH_SIZE = 64\n",
        "    dataset = load_grayscale_images('content/images_train',batch_size=BATCH_SIZE)\n",
        "    \n",
        "    t = trainGAN(\n",
        "        dataset=dataset,\n",
        "        optimizer = 'adam',\n",
        "        num_classes= 2,\n",
        "        embedding_dim= 100,\n",
        "        latent_dim=100,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=0.0002, \n",
        "        num_eg = 16, \n",
        "        epochs = 100,\n",
        "        gan_type = 'dccgan',\n",
        "        n_critic = 3,\n",
        "        clip = 1\n",
        "        )\n",
        "    \n",
        "    \n",
        "    t.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUwf0fRRnNht"
      },
      "source": [
        "## WCGAN:\n",
        "- Batch Size = 64\n",
        "- Optimizer = RMSprop\n",
        "- Learning Rate = 0.00005\n",
        "- Embedding Dimension = 100\n",
        "- Latent Dimennsion = 100\n",
        "- Number of times critic is trained = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "NFnLIDuFm0Uc",
        "outputId": "6ae800dc-d614-4bfd-c432-715a79fa6aa3"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BATCH_SIZE = 64\n",
        "    dataset = load_grayscale_images('content/images_train',batch_size=BATCH_SIZE)\n",
        "    \n",
        "    t = trainGAN(\n",
        "        dataset=dataset,\n",
        "        optimizer = 'rms',\n",
        "        num_classes= 2,\n",
        "        embedding_dim= 100,\n",
        "        latent_dim=100,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=0.00005, \n",
        "        num_eg = 16, \n",
        "        epochs = 100,\n",
        "        gan_type = 'wcgan',\n",
        "        n_critic = 3,\n",
        "        clip = 1\n",
        "        )\n",
        "    \n",
        "    \n",
        "    t.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2HBle9PnfdZ"
      },
      "source": [
        "## WCGANGP-RMSprop:\n",
        "- Batch Size = 64\n",
        "- Optimizer = RMSprop\n",
        "- Learning Rate = 0.00005\n",
        "- Embedding Dimension = 100\n",
        "- Latent Dimennsion = 100\n",
        "- Number of times critic is trained = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "o9-K4hnwnj1E",
        "outputId": "a21c2e55-1521-4a98-f5ee-3d10c0dd312a"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BATCH_SIZE = 64\n",
        "    dataset = load_grayscale_images('content/images_train',batch_size=BATCH_SIZE)\n",
        "    \n",
        "    t = trainGAN(\n",
        "        dataset=dataset,\n",
        "        optimizer = 'rms',\n",
        "        num_classes= 2,\n",
        "        embedding_dim= 100,\n",
        "        latent_dim=100,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=0.00005, \n",
        "        num_eg = 16, \n",
        "        epochs = 100,\n",
        "        gan_type = 'wcgangp',\n",
        "        n_critic = 3,\n",
        "        clip = 1\n",
        "        )\n",
        "    \n",
        "    \n",
        "    t.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3sZWSggnwDY"
      },
      "source": [
        "## WCGANGP-Adam:\n",
        "- Batch Size = 64\n",
        "- Optimizer = Adam\n",
        "- Learning Rate = 0.0002\n",
        "- Embedding Dimension = 100\n",
        "- Latent Dimennsion = 100\n",
        "- Number of times critic is trained = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "4OnS25GQnyHJ",
        "outputId": "11f5d667-32b5-45af-d708-404f5dbd0bbb"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    BATCH_SIZE = 64\n",
        "    dataset = load_grayscale_images('content/images_train',batch_size=BATCH_SIZE)\n",
        "    \n",
        "    t = trainGAN(\n",
        "        dataset=dataset,\n",
        "        optimizer = 'adam',\n",
        "        num_classes= 2,\n",
        "        embedding_dim= 100,\n",
        "        latent_dim=100,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=0.0002, \n",
        "        num_eg = 16, \n",
        "        epochs = 100,\n",
        "        gan_type = 'wcgangp',\n",
        "        n_critic = 3,\n",
        "        clip = 1\n",
        "        )\n",
        "    \n",
        "    \n",
        "    t.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Datasets and Generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj-yZ5Agobva"
      },
      "source": [
        "## Generate image sets for FID and KID \n",
        "and move them into the pre_saved_assets folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "vNn43eB3ogqS",
        "outputId": "dde14c26-951e-4099-d3cb-d4bdabf22704"
      },
      "outputs": [],
      "source": [
        "# generate image sets for FID and KID\n",
        "# >= 10000 images else FID might be underestimated\n",
        "t.generate_new_images(model = t.gen, num_eg=1024*10,ecg_type = 'AF',\n",
        "                      batch_size=64)\n",
        "t.generate_new_images(model = t.gen, num_eg=1024*10,ecg_type = 'NORMAL',\n",
        "                      batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAEYZ1Qeospi"
      },
      "outputs": [],
      "source": [
        "!zip -r for_testing.zip new_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKv6rPYHpm2C"
      },
      "outputs": [],
      "source": [
        "%cp -av for_testing.zip pre_saved_assests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk_bZWEDolcl"
      },
      "source": [
        "## Save the generator\n",
        "and move it into the pre_saved_assets folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TT5vWKloo6J",
        "outputId": "c930bdae-d753-47ae-e64e-8d2c039b61a0"
      },
      "outputs": [],
      "source": [
        "# save the generator\n",
        "model = t.gen\n",
        "model.save('test_gen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1keldFIvp3pK",
        "outputId": "60a06ba4-fbd3-4512-c325-49119efb36ea"
      },
      "outputs": [],
      "source": [
        "!zip -r est_gen.zip test_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM9x-NDHp_cZ",
        "outputId": "4e1f6a17-f403-44a0-9058-8d9f61d88a31"
      },
      "outputs": [],
      "source": [
        "%cp -av test_gen.zip pre_saved_assests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmMFhZ7txaxK"
      },
      "source": [
        "# Next:\n",
        "With the generator trained and the small image sets produced, we can now move on to the evaluation section:     https://colab.research.google.com/drive/1b38zrwvoWTGgBm_PwrP2Sp_XGu9Ixfhg?usp=sharing"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "Nac1WsLmcjCn",
        "6HFZZgasctgo",
        "nQlFI-2Zux6L",
        "JlMiYLylc5Jc",
        "TCOBDBMcc-Vs",
        "BLl1_mZTdETL",
        "nZRyq2TsDFLe"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "cc414e0b3354e9e15d1dfba118355067b70b63730e80deb65fb9da2ad81d92c8"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0713b0702f3c482980151392bef4e585": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92db24aa4148468f91b65322e25cdb4f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b6aeb4a6c054cba82e9892156a7f275",
            "value": 1
          }
        },
        "0aa8e05a9a884a8e88fa3a4d20d576a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "225d893b51ed4791a6c63340279df8ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe92400cfc14189aedeb60de1f23d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3063da153b7e4bc6ad836584eed39543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc52dbf1fa304d7a8228c14cfe5f425d",
              "IPY_MODEL_0713b0702f3c482980151392bef4e585"
            ],
            "layout": "IPY_MODEL_8430e568342340c89bf81b98ba9704f3"
          }
        },
        "47ce07063b7843e5926f7ad03e402789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53bd932307824af0873a90fc6af959e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a139bec6294a7792bb66bd8beb2fd5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fe92400cfc14189aedeb60de1f23d3a",
            "value": 1
          }
        },
        "75b20fbffc5048bd851a3ba804701bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f4e7c5ad85a4b988996f39c861f6c72",
            "placeholder": "​",
            "style": "IPY_MODEL_0aa8e05a9a884a8e88fa3a4d20d576a8",
            "value": "0.481 MB of 0.481 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "7d6d02d6902740f7b063aff992b713b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f4e7c5ad85a4b988996f39c861f6c72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8430e568342340c89bf81b98ba9704f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92db24aa4148468f91b65322e25cdb4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b6aeb4a6c054cba82e9892156a7f275": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f3963c11e704fd7b969087532514bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75b20fbffc5048bd851a3ba804701bf4",
              "IPY_MODEL_53bd932307824af0873a90fc6af959e1"
            ],
            "layout": "IPY_MODEL_7d6d02d6902740f7b063aff992b713b8"
          }
        },
        "cc52dbf1fa304d7a8228c14cfe5f425d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_225d893b51ed4791a6c63340279df8ff",
            "placeholder": "​",
            "style": "IPY_MODEL_47ce07063b7843e5926f7ad03e402789",
            "value": "0.113 MB of 0.113 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "d0a139bec6294a7792bb66bd8beb2fd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
